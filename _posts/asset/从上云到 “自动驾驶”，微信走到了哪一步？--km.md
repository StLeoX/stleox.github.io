> ​	为什么国内的Tencent没有Google那样宏伟的影响力，很大一部分就是因为没有借助**开源的力量**。
>
> ​	下面这个典型案列，同样是` Resource Disperser `这个方向的产品，Tencent的yarp 和 Google的k8s 项目最后所取得的影响力是存在云泥之别的，很大一部分原因就是yarp只是Tencent内部在开发、维护、反馈，而k8s则参与到了云原生产业标准的制定，所以k8s才能越来越完善，并逐渐成为产业标准。



### “年三十晚上抢不到红包，我们就会被所有人的口水淹死。”

回忆起 2019 年底春节微信红包保障的惊险一幕，微信技术架构部负责人 Stephen 依旧神色紧张。

对于这款国民级应用，微信全年最高峰值发生在除夕零点的那一刻，全国人民发红包、抢红包，迎接新一年的到来。按照惯例，为了扛过流量洪峰，微信团队需要在除夕前加很多服务器。2019 年 10 月，恰逢腾讯云推出国内首个 AMD 大规模产品化的星星海云服务器，借助自研上云的机会，推动内部自研团队作为 AMD 星星海的第一批用户，凭借强劲的性能、更低的服务器成本，很多内部业务纷纷试用，微信就是其中最大的内部客户。

尽管 2019 年初，微信的收发消息、朋友圈等功能模块已逐步跑在云服务器上，但 2020 年这场春节红包保障，**是微信第一次大规模使用云上服务器**。

在部署完新机器并系统扩容后，微信团队就会做攻击性模拟测试，基于往年除夕零点的数据，预测今年增加的数量，进行完全模拟压测。这时团队发现，在一些常见的模块上，云服务器表现还算正常，但当压测到一些非常关键的模块，云服务器在 CPU 负载较低的时候，服务就会出现失败。

Stephen 回忆，“直到 1 月中旬，还没压测到我们想要的数值，大概每分钟下发几十亿条消息数，但压测的水平只有目标的一半。” 由于压测不是在一个完全独立的环境下，“压测到服务器失败，我们就不压了，再压下去用户就会感知到。”

而此时距离春节只有两周时间了，**“如果这个问题解决不了，导致大家在大年三十晚上抢不到红包，我们就会被所有人的口水给淹死。”**

危急关头，微信和 CVM 云团队紧急联合排查，发现问题出自网络虚拟化导致的网卡性能下降。问题找到了，但在这么短时间内做网卡的性能优化也来不及了。经过反复商量，大家想到一个方法，“微信有各种不同负载的服务，可以把云服务器切分成更小的分片，让负载上到 50% 至 60%，这个时候网卡还没有出现失败，这样就可以将一台机器上剩下来的 CPU 部署到负载较重的模块上，另外做一个组合。”Stephen 分析到。

其实，这也是云平台本身的灵活性所在。在星星海的保障下，微信和 CVM 团队齐心协力扛过了 2020 年的那个春节。



### “究竟用自研的，还是用开源的？”

自研上云的过程并没有想象中容易。

2014 年初，微信还只是一个部门，三四百号人都扑在不断打磨产品新功能上，对于后台服务器利用率如何、架构做得怎么样却极少关注。那一年，Martin 和总办团队向全公司提出成本优化的任务，微信内部技术实力强劲的架构优化团队基于云平台打造了自研的 yard 系统（yet another resource disperser），即 “仅仅是另一个资源分发系统”，最初就是为了让服务器更加灵活地调度，达到节省成本的目的。

彼时，早期的 K8S 系统在 2014 年 6 月份才开源，而微信自研的 yard 系统已初具成效。“究竟用自己自研的，还是用开源的？” 团队当时也颇为纠结，最后出于对自研系统可控性的考虑，加上当时 K8S 成熟度还不够，微信最终选择自研的 yard 系统。

不过到了 2017 年，当微信开始做离线混布的时候，yard 的局限性就显示出来了。当 yard 提供离线训练能力给业务团队的时候，发现各个团队都有不同的喜好，比如大数据挖掘喜欢用 mapreduce 或 spark 技术、AI 训练喜欢用 tensorflow 或 pytorch 技术，所有离线训练的框架都要一个个去做适配，这无疑给团队带来巨大的工作量。

面对这一局面，微信技术架构部想到，能够找到一个有影响力的开源云平台，让不同的训练框架主动适配，就可以把这些工作量节省下来。2017 年，从 Google 内部发展出来 K8S 系统不论在规范上还是功能上，都比 yard 系统要强很多，比如对虚拟化的支持，yard 系统只支持了 CPU 和内存的虚拟化，而 K8S 会在磁盘、网络等部分都支持得更加完整。考虑到业务也有诉求，大家一拍即合，决定干脆将 yard 整个切换到 K8S。

恰逢 2018 年腾讯 930 战略和组织架构变革，同时宣布成立技术委员会，提出 “开源协同，自研上云” 的技术战略，微信已经是非常拥抱的态度了。在微信，stephen 负责的技术架构部，是微信内部唯一一个不负责具体业务，提供公共技术支撑的部门。微信上云的重任，便由这个团队推动实现。

**“这个问题不解决，微信就没法上云。”**

自研上云起步之初，K8S 也才刚刚启动共建。整个公司自研上云的过程分为两个阶段，第一阶段是将 CVM 虚拟机替换掉过往的物理服务器，直到 2019 年下半年到 2020 年初，才开始真正做 K8S 容器化改造。

2019 年初，由 CSIG 和 TEG 组成的 CVM 虚拟化云产品团队，为公司自研业务提供了首个自研上云专区支持，一开始只是简单地将公有云上的标准化配置交付给自研业务去使用，然而，当面对腾讯内部各种复杂的自研业务，一些问题也逐渐浮出水面。

**“你们的 CVM 怎么比物理机差这么多？”**

这是早期云产品同学听到最多的内部 “吐槽”。自研业务的一个特点，会做地毯式的功能验证和极为严格的测试，微信就是拿 yard 系统的各种典型功能模块去测试，看看跑在虚拟机上面的各项功能指标是否能被满足。

由于自研业务完全从过去的物理机环境迁移上云，所以会有非常直观的对比。微信对 CVM 团队提出非常明确的要求：云上的 CVM 功能相比于物理机的性能损耗要控制在 8% 以内。但当时 CVM 只能做到在不同场景下虚拟化损耗在 15% 左右波动，这对云产品团队来说是完全达不到的目标。

**“这个问题解决不了，我们没法上云。”** 微信等业务团队也提出了明确的要求。

其实对于公有云的通用性能工具，相差 15% 已经属于业界比较好的指标，但还是难以满足自研业务的细分场景需求。

“我们其实也被降维打击了。”

CVM 团队发现，之前的测试场景确实有一些盲区没有被覆盖到，这也是内部自研业务做的非常专业的地方。比如，之前只测单台云服务器的性能，而自研业务会要求把所有的云服务器放到一台物理机上同时压测，看互相会不会影响；自研团队也会根据自身实际业务特点，细化到具体消耗哪方面性能较多，基于此提炼出模型，给出测试报告，去做云服务器和物理机的对比。

经过三个月左右的磨合和优化，CVM 团队针对业务提供的场景做了全面的优化，并定制了几款优化产品先提供给自研业务使用，最终虚拟机和物理机的性能损耗控制在 5% 以内，在 2019 年中旬解决了性能匹配自研业务的需求。CVM 团队还惊喜地发现，这些给自研业务提供优化方案也可以覆盖到公有云，作为云的产品化的能力向外输出给客户，让整个公有云上的新机型都具备这样的能力。

**“K8S 对三园区容灾能不能做到？能不能做好？”**

2020 年，上云的重心转移到容器化。技术委员会给各个 BG 分派了上云的量化指标，WXG 接到了一年内完成 20 万核的上云目标。

“总体来说比较懵”，谈到刚接到上云任务的时候，负责 yard 平台 Poseidon 容器化开发的 edselwang 这样形容。

虽然 K8S 已经逐渐成为云的标准，但当时大家对 K8S 的了解仍处在比较浅显的阶段，如果将原来的一套系统对接集成到 TKE，那么就意味着微信业务的核心模块，如收发消息，发朋友圈都需要上云。对于这款国民级产品来说，任何一个小的改动，影响面都非常巨大。



### Poseidon 云平台 2.0 上线

Stephen 总结了微信上云的 “三步法”：第一步在微信内部叫做 “冒烟测试”，至少将各种功能在 K8S 上跑一遍，测试它的兼容性。第二步，就是把 yard 系统上积累的经验移植到 K8S 上。第三步，充分挖掘 K8S 在网卡、磁盘的虚拟化的调度的能力，让其更好地跟微信业务结合。

时间紧任务重，edselwang 和开发、运维同事们每天都会聚在黑板前讨论、上线、测试、bugfix，“那段时间大家还是比较焦虑的”。第一个上线的功能是 “查看附近的人”，在灰度阶段，先调动一些小的功能去试，尝试过之后，才比较放心地上一个个核心重头的模块。

上云的过程是一个迁移的过程，**在迁移过程中保持系统的稳定性，是微信最关注的点**。一旦迁移到 K8S 发生问题，每一步都可以立刻回退，并做成一个常规功能，这让整个迁移过程的风险就变得非常可控。

微信历史以来最大的一次故障发生在 2013 年，当时上海机房被挖掘机挖断了光纤，导致整整 5 个小时微信无法使用。那次故障之后，，微信在 yard 上设计一个很重要的功能，叫做 “三园区容灾”，即在部署的时候，要求在一个城市内部要有三个园区，园区的概念就是一栋建筑物，相当于三栋建筑物是三个机房，在电力和网络上要互相独立。

**“所以我们当时从 YARD 切换到 K8S，首要考虑的就是 K8S 对三园区的支持能不能做到、能不能做好。”**WXG 技术架构部的 Stephen 说。

**除了 yard 迁移到 K8S，还有一个标志性的事件就是存储上云方面也有了突破。**由于微信有非常多极端的要求，比如朋友圈的特点是一个人发表，有两三百个好友查看到，这个量其实非常大，微信做了很多定制化的优化。目前整个微信的核心数据还是使用自研的存储系统，在过去的 10 年，存储团队经历了 SimpleDB、QuorumKV、PaxosStore、InfinityKV 的进化，最终在 InfinityKV 这个版本实现了存储上云的能力，从 2020 年下半年开始上线，目前微信后台 80% 的数据已经存储在这个新的系统，把系统改造成支持云的能力，使用云提供的服务器，脱离掉原来的物理机。



### 存储上云的研发团队

**“希望未来整个研发流程按照云原生的方式去做”**

对于技术人员来说，推动自研业务上云不是一项单纯的 “政治任务”，不论是开发、运维、测试等不同技术角色的同学们，都看到了其中的价值，主动拥抱上云。

在公司提出自研上云这几年，微信内部也讨论团队的上云目标。stephen 说，**“我们希望不止是配合公司完成在虚拟机和容器上的上云指标，希望未来整个研发流程按照云原生的方式去做。”**

“在原来的研发流程里，对于持续集成、持续部署的 CICD 并没有做得太好，一方面因为当时没有特别强的诉求，更重要的原因是 yard 对于虚拟化的支持还不够好，基于 yard 做 CICD 的优化还有很多欠缺。现在上云后，在 IaaS 和云平台的成熟度越来越高，也促进整个研发在 CICD 上跟进行业的最佳实践。”

一年多以前，Stephen 还跟团队分享过一个观点，他拿自动驾驶的 6 个 level 做类比，level0 是人工驾驶，完全没有自动化、level1 是有一些驾驶辅助、level2 是更强的驾驶辅助、leve3 已经具备一定的自动驾驶能力了，还有 level4、level5、level6。几年前我们只是在 level0，2014 年有了 yard 之后是 level1，经过 2021 年挖掘 K8S 的各种能力之后，现在应该处在 level2 的状态。

**“我希望接下来能够达到到 level3，希望未来云原生能够达到像自动驾驶一样，现在每年的春节保障相当于是人工在驾驶的，未来希望完全由机器自动驾驶完成。”**

