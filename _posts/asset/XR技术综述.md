[VR/AR/MR/XR 概念辨析](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI2NjkxMjkxNg%3D%3D%26mid%3D2247512306%26idx%3D1%26sn%3D5e245e903bcabc378288f8a21e273909%26chksm%3Dea842509ddf3ac1fdfffb30598f417dc1e677451f860ab79748193dfae042f2867cb98211901%26scene%3D21%23wechat_redirect)，了解到扩展现实（XR）实际是一个概括性术语，囊括了增强现实（AR），虚拟现实（VR），混合现实（MR）以及介于他们之间的所有内容。

![](https://pic2.zhimg.com/v2-3d2bcf210313f89395e456cb9da090a1_r.jpg)

这一期我们一起梳理一下 XR 领域的关键技术。虽然在体验上 AR 和 VR 差异很大，但是两者却有着相同的技术基础，多领域技术交叉重合，而 MR 一般被理解为 AR 能力的增强，和 AR 技术栈高度复合，国内一般统一把他们作为虚拟 / 增强现实领域一起分析，本文也采用了这种分析方式。

当前 XR 相关技术和产品还在发展期，信通院在其白皮书中给出的范畴比较全面，因此在此引用了其技术体系展开 [2][4]。首先在顶层定义了“五横两纵” 技术架构见图 2。“五横”是指近眼显示、感知交互、网络传输、渲染处理与内容制作五大技术领域。“两纵”是指支撑虚拟现实发展的关键器件 / 设备与内容开发工具 / 平台。

![](https://pic4.zhimg.com/v2-5abcbc1cf6bb45ab0a47576b5e00ff03_r.jpg)

横向技术维度可细分为三层体系，第一层为五类技术领域，每个领域可再细分子领域和技术点，见图 3 示意。**这种分类方法可能不是最有深度的，但是相对是最全的**。本文后面会对每个热点技术展开基础了解，如果深入每个点都是一个独立的技术领域。

![](https://pic2.zhimg.com/v2-ed5db7ace0762b21bdaf57c591a8833d_r.jpg)

XR 热门技术点成熟度曲线见下图，很多技术都处在爬坡期。技术供需面临多重挑战，存在超长的产业链条致使创新投入力不从心，现实效果与用户预期存在落差等问题。根据虚拟现实产业推进会（VRPC）产业分析与体验调优平台数据统计，用户体验痛点清单按优先级排序可归纳为 “**用贵笨视晕传知**”，即高品质爆款**内容缺乏**；高性能终端存在一定**价格门槛**；外观形态吸引力不足，佩戴**不够轻便**；分辨率、视场角等方面的画面**视觉质量有限**；**头动响应（MTP）时延**、**辐辏调节冲突（VAC）**。

![](https://pic1.zhimg.com/v2-19366010edf73df0c8a9d06edec689ec_r.jpg)

为了度量 XR 的发展阶段，参考国际上自动驾驶汽车智能化程度分级信通院将虚拟现实技术发展划分为如下五个阶段，得到国内产业内一定的认同，指标见下图。

![](https://pic4.zhimg.com/v2-57653a5104eff43ff4b487c860003cdb_r.jpg)

根据以上分级，我们当前的水平处于部分沉浸期，主要表现为 1.5K-2K 单眼分辨率、100-120 度视场角、百兆码率、20 毫秒 MTP 时延、4K/90 帧率渲染处理能力、由内向外的追踪定位与沉浸声等技术指标，正在向深度沉浸过度。下面我们就分领域讨论核心技术点的内容。

**近眼显示**
--------

没有头戴设备的 XR 应用场景，沉浸感无从谈起，XR 头显 / 眼镜的近眼显示技术是沉浸感提升的前提。

在具体介绍技术点之前先说一下基础概念视场，它主要表示人眼所能看到的图像最大角度范围。一般人的话，我们**水平方向双眼是 200 度**，会有 **120 度的重叠**。双眼**重叠部分对于人眼构建立体和景深非常重要**，而**垂直视角大约为 130 度**。

![](https://pic4.zhimg.com/v2-dee1871463fbaf0fe3c3a5e95d318d77_r.jpg)

再介绍一下显示方式的分类，当前主要分三种。**完全沉浸型**，传统的 VR 都是这种情况，和现实完全的隔离的显示方式；**光学透视型**，当前主流的 AR/MR 眼镜都是这种类型；**视频透视型**，把显示场景通过摄像头视频的方式呈现在用户眼前，带 MR 能力的 VR 眼镜是这种类型，是当前 VR 眼镜的主流发展方向，如 oculus quest2 有四个摄像头，在 MR 场景下可以比较清楚的 “看” 到周边的环境。从显示原理上看我可以粗分为 VR 类型（非透视型显示）和 AR 类型（透视型显示）两种，后面我们就以 VR 和 AR 两种场景进行分析。

![](https://pic1.zhimg.com/v2-8961b0c7d1684ca1b1baaaf19a64dd08_r.jpg)

近眼显示是 XR 的沉浸感提升的核心技术，一直备受瞩目。但是受限于核心光学器件与新型显示的发展，整体相对迟缓。2020 年随着市场需求日渐清晰，业界对近眼显示领域表现出更高的期待。

**1.1 显示器领域**

VR 类型非透视型和 AR 类型光学透视型显示对应当前两种主流显示器类型，**快速响应液晶**（Fast-LCD）和**硅基 OLED**（OLEDoS），处于实质规模量产阶段。

VR 类型显示器首选 Fast-LCD。2020 新 VR 终端多采用 Fast-LCD，如 Facebook Quest 2 因性价比替换了上代产品中 AMOLED。

**AR 类型**显示器当前首选为 OLEDoS，在对比度、功耗与响应时间等方面的性能表现都能达到要求。而 **LBS 激光扫描**显示被应用在微软等高端产品，亮度、功耗与体积等方面的优势使得该技术获得业界关注，但需搭配较为复杂的光学架构实现功能，且虽然技术先进，但最终在分辨率、偏色等方面表现一般，应用前景不明。

**微型发光二极管**（Micro-LED）适用于以上两种显示类型，是未来的发展方向。Micro-LED 具备低功耗、高亮度、高对比度、反应速度快、厚度薄与高可靠等性能优势，但现阶段 LED 受限于工艺问题未能量产，根据当前产业相关发展情况预计其规模量产时间在 2022 年左右。2020 年 Mojo Vision 发布了首款内置 Micro LED 的 AR 隐形眼镜，当前智能隐形眼镜尚处于萌芽状态。

未来，**近眼显示系统有望由当前眼球外安置（头显终端 / 眼镜）向眼球上（隐形眼镜）、眼球内（晶状体、视网膜）乃至视觉皮层转移**。

**1.2 光学领域**

在光学领域，发展方向是以人为中心的光学架构，视觉质量、眼动框范围、体积重量、视场角、光学效率与量产成本间的权衡取舍、优化组合成为驱动技术创新的主要动因。

VR 领域难度低比较成熟。当前超薄 VR（Pancake）利用半透半反偏振膜的双透镜系统折叠光学路径，将头显重量降至 200g 以内，且可保证较好的显示效果及更大的视场角。

AR 领域难度大发展相对缓慢。**折反式**（Birdbath）设计因难度低和成本低是当前消费级 AR 的首选，但其厚度问题导致其未来发展空间小。**自由曲面**在早期得到业界认可，其显示效果、光效表现较好，但量产难以保证较高精度而导致真实世界扭曲和水波纹样畸变，发展前景也不被看好。**光波导**相比其他光学架构，光波导外观形态趋近日常眼镜，且通过增大眼动框范围更易适配不同脸型用户，有助于推动消费级 AR 产品显著升级，是 AR 领域主流技术。

**波导的概念**，顾名思义，它是一种物理光学结构设计，可以使光线曲折进入人的眼睛。这被用于内部的反射以及光线进出的控制，工业上有四种波导结构设计。

**全息波导** (Holographic waveguide) 这是光学元件中的一种简单的波导类型，例如用于通过一系列内部反射进行耦合（进入）和外耦合（退出）。这种类型用于 Sony 的 Smart Eyeglass。

![](https://pic2.zhimg.com/v2-50ec08facb079eb35544b46661ab5a15_r.jpg)

**衍射波导** (Diffractive waveguide) 精准的起伏表面起伏光栅用于实现内部反射，从而通过显示器实现无缝 3D 图形的覆盖。这些波导用于许多 Vuzix 显示设备和 Microsoft 的 Hololens 中。

![](https://pic1.zhimg.com/v2-ec0b4a911530df219c6fd73a6c2c9bd4_r.jpg)

**偏振波导** (Polarized waveguide) 光进入波导并通过部分偏振表面上的进行一系列内部反射。选定的光波会抵消（偏振）并进入观看者的眼睛。该方法由 Lumus DK-50 AR 眼镜使用。

![](https://pic2.zhimg.com/v2-d4f351383c724ca2583a1ed8d389b1ed_r.jpg)

**反射波导** (Reflective waveguide) 类似于全息波导，其中单个平面光导与一个或多个半反射镜一起使用。在爱普生的 Moverio 和 Google Glass 中都可以看到这种波导。

![](https://pic4.zhimg.com/v2-770f788529095b3abf252b4e35db12db_r.jpg)

**当前衍射光波导理论上具有较高的可加工性，成本可控，批量生产难度显著低于阵列光波导，现已成为国内外标杆企业研发创新的活力区。**

**1.3 眩晕控制**

发展符合人眼双目视觉特性的近眼显示技术成为虚拟现实眩晕控制的技术制高点。从人眼双目视觉特性看，业界公认的眩晕感主要源自三方面。一是显示画质，纱窗、拖尾、闪烁等过低的画面质量引发的视觉疲劳容易引发眩晕，提高屏幕分辨率、响应时间、刷新率，降低头动和视野延迟（MTP）成为技术趋势。二是视觉与其他感官通道的冲突，强化视觉与听觉、触觉、前庭系统、动作反馈的协同一致成为发展方向，目前除前庭刺激、服用药物等非主流方式外，全向跑步机成为缓解此方面眩晕感的主要技术。三是辐辏调节冲突（Vergence Accommodation Conflict，VAC），由于双目视差在产生 3D 效果的同时，造成双目焦点调节与视觉景深不匹配，VR 头显难以如实反映类似真实世界中观看远近物体的清晰 / 模糊变化。目前，可变焦显示（Vari-focal Display）成为解决 VAC 问题的重要技术，Facebook 已经应用到其产品，并在不断优化，有望极大程度的优化了头显体积重量与系统可靠性。还有全息显示也是解决 VAC 的技术路径，但是当前技术成熟度低。

**总体我国在近眼显示领域与国际一流水平差距不大，需强化部分前瞻领域技术攻关。**

**内容制作**
--------

作为新一代人机交互界面，虚拟 / 增强现实契合时下新媒体所追求视觉沉浸感与用户交互性的发展趋势。虚拟现实内容制作技术开始广泛应用，在 “采、编、播” 以及交互等环节注入了创新活力。

**2.1 内容采编播**

**内容采集环节**，由于虚拟、增强现实可提供 360 度、720 度的全景视频，需要 360 度拍摄，编导与摄影师等工作人员站位、观众视觉兴趣点引导、多相机同步控制等新问题对内容采集带来挑战。用于**全景拍摄**的相机可分为**手机式、一体单目式、一体多目式、阵列式、光场式**等。全景相机发展呈两极化演变态势，一方面为方便更多 UGC 快速便捷的制作虚拟现实内容，会朝着小型化、易用化、多功能、机内拼接、降低成本方向发展。另一方面为满足高端 PGC 生产高质量视频内容，更高分辨率、自由度、更多视频格式与斯坦尼康等拍摄辅助器材支持成为又一发展路线。**全景声麦克风**（Ambisonic）可以采集单点所有方向的声音，作为一项既有拾音技术随着虚拟现实的兴起被业界关注，目前谷歌、Oculus 已将其作为 VR 的声音格式。

**内容编辑环节**，由于虚拟现实相机涉及多镜头同时拍摄，从而产生出视频间**精准拼接缝分割内容**编辑技术。根据实现方式的不同，可分为实时、离线拼接与自动、手动拼接等。英伟达推出其拼接编辑软件 VRWorks360，可实现单一 VR 相机中多达 32 个拍摄镜头的跨平台的实时拼接。除全景视频所须的拼接分割外，为进一步增加内容互动性与社交性，可通过**虚拟化身**技术实现以机器或是以真实用户为对象的模拟，后面的交互体验部分会详述。

**内容播放环节**，由于虚拟现实需要解决如何将内容编制时的平面媒体格式转化为用户最终看到的全景球面视频，因此运用了传统视频没有涉及的投影技术。其中，等角投影是 YouTube、爱奇艺等采用的主流技术，但存在画质失真、压缩效率低等问题，多面体投影成为发展方向。

**2.2 平台技术**

**操作系统**有挑战。相比手机 OS 对于虚拟现实用户姿态变化难以做出实时性响应，虚拟现实 OS 不论用户主动操作与否，从姿态到渲染保持稳定运行，MTP 时延约束成为实时性挑战。由于虚拟现实空间可极大延展，支持用户同时可见更加丰富信息，操作系统多任务特性成为必然需求。在三维系统中的多任务化须实现系统多应用的三维化合成，在虚拟现实空间中布置各应用的运行位置，并实现 3D 交互，如微软 Hololens、Facebook Quest 等代表性终端对操作系统三维化多任务运行的支持。2020 年虚拟现实操作系统持续演进，VR、AR OS 在感知交互方面日渐趋同，基于计算机视觉成为发展重点，Facebook 发布 Oculus Quest 系列验证了计算机视觉实现的可行性和准确性，但是挂载 4 颗以上实时性要求较高的摄像头，操作系统亦须适配调优。

**WebXR 生态发展**。2020 年 7 月 W3C 发布新版 WebXR 规范草案，与此前 WebVR 相比，WebXR 新增了对 6DoF 追踪定位、交互外设与 AR 应用的支持，多个网页开发框架均已支持。当前，内容不足是 XR 面临的主要痛点，内容生态效率受制于碎片化的软硬件平台的影响，2019 年 7 月 Khronos 对此发布了 OpenXR 旨在实现内容应用无须修改移植即可跨头显平台运行。同时，OpenXR 强化了对 WebXR 网页开发框架的支撑，适配了手势、眼动追踪等多元化交互方式，丰富了 5G 边缘计算等应用场景。在操作系统方面，实时性、多任务、感知交互与端云协同成为当前发展焦点。

**云化虚拟技术**。对于云化虚拟现实业务需求，如何同步终端和云端数据成为操作系统技术演进焦点，如微软推出 Hololens 云方案，用户可在云端记录三维地图扫描信息。开发引擎方面，基于 OpenGL ES 底层框架，面向移动设备的低功耗、可视化开发引擎助力 VR 应用开发效率提升。对于移动虚拟现实设备，如何平衡性能和功耗成为选择虚拟现实开发引擎的关键因素。

**2.3 交互体验**

从用户与内容应用间的交互程度看，虚拟 / 增强现实业务可分为**弱交互**与**强交互**两类。前者通常以被动观看的全景视频点播、直播为主，后者常见于游戏、互动教育、社交等形式，内容须根据用户输入的交互信息进行实时渲染，自由度、实时性与交互感更强。

在**弱交互领域**，虚拟 / 增强现实视频的社交性、沉浸感的提升，强弱交互内容界线趋于模糊。当前体育赛事、综艺节目、新闻报道与教育培训等直播商业落地相对成熟。VR 直播新形式根据交互体验自由度划分，虚拟现实视频可分为基于视野转动的 3DoF、面向狭小空间内有限移动的 3DoF+、房间级一定空间内 6DoF - 及多房间或超大开放空间中的 6DoF 视频。与现阶段 3DoF 视频相比，六自由度视频摄制技术（3DoF + 及以上）可大幅提升虚拟现实用户体验沉浸性。预计未来三年，可适配高质量六自由度的内容采集系统、摄制表现手法、云网端支撑环境、场景表示与编解码算法等细分领域将成为潜在挑战及有关标准工作的推进方向。此外，相比传统无交互视频中单视角单结局、既往轻交互 VR 视频中多视角单结局的表现形式，个性化 VR 视频除呈现多视角多结局、叙事线进程可变的特点，即 “你在看视频，视频也在看你。

在**强交互领域**，VR 社交成为游戏以外重要应用场景，**虚拟化身**正在拉开虚拟现实社交大幕。VR 场景对虚拟化身的感知与控制构成了交互闭环，即追踪采集的用户数据被实时投射于虚拟化身外观及行为表现。得益于 3D 沉浸视频、超大视角及进阶追踪能力，位置、外貌、注意力、姿态、情绪等日益多元精细的身态语汇激活了虚拟化身潜藏的社交表现力。通过营造多人共享的临场感，VR 社交进一步放大了虚拟现实强交互业务的互动程度，并结合日常交流所须的适宜间距、注视转头、手势表情等潜藏的通识准则来优化虚拟化身。如何持续提高虚拟化身真实感，同时精准调和外貌与行为拟真度间的配伍关系，成为 VR 社交虚拟化身的主要技术挑战与发展方向。

在技术选型上，基于**口、眼、表情、上肢拟真**等的虚拟化身技术初步成熟，现已开始用于 VR 社交应用。**口型方面**，依托三维扫描人类发声时对应的面部拓扑特征，构建包含广谱语音口型的模型库，借助机器学习训练音画同步网络，通过语音实时驱动面部动画。当前存在问题是口型如声音的匹配问题，业界通过解构不同语音对各面部肌群的协同牵引关系，旨在发展出更加自然可靠的音画同步技术。**眼动方面**，虚拟化身可精细模拟一系列眼动眼神行为，如下意识眨眼、交谈间注视、移动物体追视、多物体快速扫视、饱含情感凝视及特定情况下瞳孔放大、视野舒适区外转头等情景，进而极大程度地丰富了 VR 社交的表现力与真实感。预计未来三年，除现有口型、眼动、微表情、手势肢体等上半身虚拟化身细分领域的优化迭代外，**全身型虚拟化身有望兴起**。

总体看，我国在内容制作方面与国际一流水平各有所长，须强化对部分重点领域的技术攻关。

**感知交互**
--------

感知交互强调与其他领域的技术协同，各大巨头与初创公司对此深度布局，积极投入。当前，**追踪定位、沉浸声场、手势追踪、眼球追踪、三维重建、机器视觉、肌电传感、语音识别、气味模拟、虚拟移动、触觉反馈、脑机接口**等诸多感知交互技术百花齐放，共存互补，并在各细分场景各具优势。未来，理想的人机交互可让虚拟 / 增强现实用户聚焦交互活动本身，而忘记交互界面及手段的存在，越来越 “透明”，**自然化、情景化**与**智能化**成为感知交互技术发展的主要方向。

**3.1 追踪定位**

追踪定位是感知交互的基础和前提，只有确定了现实位置与虚拟位置的映射关系，方才进行后续诸多交互动作。追踪定位技术呈现由外向内的空间位姿跟踪（Outside-In Position Tracking）向由内向外的空间位姿跟踪（Inside-Out）的发展趋势。

当前 Inside-out 技术全面成熟，追踪定位将呈现集视觉相机、IMU 惯性器件、深度相机、事件相机等多传感融合的发展趋势。在 **VR 领域**，存在 **outside-in 和 inside-out 两条技术路线**。通过超声、激光、电磁、惯导等多种传感器融合定位较单一惯性和光学定位减少了计算资源消耗，在一定程度上优化了功耗与鲁棒性表现。目前，基于视觉 + IMU 的 inside-out 追踪定位技术实现产品化，开始大量应用于头显终端，代表产品有 Oculus Quest1/2、HTC Vive Focus 等。**在 AR 领域，Inside-out 是唯一主流技术路线**，基于终端平台的差异，视频投射式 AR（video see-through）以苹果 ARKit、谷歌 ARCore、华为 AREngine 以及商汤 SensAR 为代表的 AR SDK 普遍遵循单目视觉 + IMU 融合定位的技术路线，在 2019 年对其跟踪精度和鲁棒性进行了进一步提升，毫米级别的定位精度使得 AR 尺子等空间测距等应用大量出现。光学投射式 AR（optical see-through）以微软 Hololen2、Magic Leap One 为代表的 AR 眼镜普遍遵循双目 / 多目视觉 + IMU 融合的技术路线，可提供毫米级别精准度的定位输出和世界级规模的 6DoF 追踪定位，其中 SLAM 算法的稳定性主要受光线与环境复杂程度影响。由于室外光线会影响到摄像头的使用，Oculus 在黑暗条件下难以提取环境信息，从而影响 SLAM 结果。Hololens2 采用 TOF 提供**主动光辅助定位**，在一定程度上缓解了该问题。环境复杂度表现为 AR 眼镜受限于摄像头可实现高精准度获取信息的范围限制，在过于空旷（无参照物）的环境中，难以实现厘米级别定位。此外，随着基于神经拟态视觉传感器（dynamic vision sensor）的事件型相机技术发展，利用其高帧率、抗光照等特性，追踪定位技术鲁棒性有望进一步提升。

**手势追踪**初步成熟，将成为虚拟现实输入交互新模式。手势追踪技术的价值优势在于手是天然的输入工具无需购买链接设备，且手势信息等身态语表现力强，赋予了内容开发者更大的创作空间。当前基于黑白 / RGB 摄像头的机器视觉技术路径已成为标记点、3D 深度摄像头方案外手势追踪的重点实现方式。当前，手势追踪技术在多维发展方向上初步成熟。在算法鲁棒性优化方面，通过收集用于深度学习的多类人群手势及环境数据，可探知手部位置及关节指尖等特征点信息，进而结合反向动力学算法构建手部 3D 模型。在计算及功耗开销控制方面，通过深度神经网络量化压缩技术，精准可靠的手势追踪算法得以在移动式虚拟现实终端上（一体式、手机伴侣）以较低算力、时延与功耗预算运行。在交互表现性探索方面，时下业界围绕人因工程视角，就输入交互进行创新设计，以 “捏” 代“按”，可有效节省交互空间，明确交互起止时点，获知输入反馈。除单手追踪外，双手、手与笔、手与键盘、手与控制器等外设配合成为手部交互表现性探索的新方向。

**眼动追踪**成为虚拟现实终端的新标配。早期虚拟现实终端以头动追踪为主，当前用户需求开始对眼动追踪提出了更高要求。眼动追踪主要涵盖**注视点追踪、瞳孔位置尺寸追踪、眼睑数据采集、生物识别**等，得益于该领域在虚拟现实融合创新与以人为中心研发思路上的技术潜力，眼动追踪日渐成为 VR/AR 终端的新标配，且应用场景趋于多元。例如，注视点追踪可用于眼控交互、可变注视点渲染与注视点光学、FOV 一致性补偿、可变焦显示系统中的辐辏调节冲突控制等任务场景。眼动追踪技术主要分为基于特征与基于图像的发展路径。两种方案均须红外摄像头与 LED 完成，前者通过光线在角膜外表面上普尔钦斑（Purkinje image）反射以推算瞳孔位置，已成为业界主流技术方案。当前，眼动追踪技术发展的难点在于眼动算法如何基于所采集的原始眼动行为来 “透视” 用户意图。此外，除追踪精度指标外，用户个体与环境差异（眼球角膜、佩戴眼镜、周围光线等）对系统通用性提出了更高要求。

**3.2 环境理解**

**环境理解以及 3D 重建将成为虚拟现实感知交互领域技术内核之一**。环境理解呈现由有标识点识别向无标识点的场景分割与重建的方向发展。相比 VR，AR 大部分视野中呈现真实场景，如何识别和理解现实场景和物体，并将虚拟物体更为真实可信的叠加到现实场景中成为 AR/MR 感知交互的首要任务，而基于机器视觉的环境理解成为这一领域的技术焦点。

在 AR 应用的早期，绝大部分 AR 引擎通过获取图像中标识点（Marker）的特征信息并与预存的模板进行匹配，来识别当前 Marker 的种类及位置信息，Marker 从如 ARToolkit 等有明确边缘信息和规则的几何形状演进到任意图像，这类基于标识点的识别技术使用限制较多，应用场景较为狭窄。随着深度学习和即时定位与地图构建（SLAM）等识别和定位重建技术的发展普及，未来的 VR/AR 将不仅局限在对特定 Marker 的识别，而会**逐渐拓展到对现实场景的语义与几何理解**。在语义理解方面，主要任务是利用卷积神经网络（CNN）对单帧图像或连续多帧视频中所出现的物体和场景进行识别和分割，大致分为分类、检测、语义与物体分割，即确定图像中物体类别、大概位置、物体基本边缘轮廓以及针对分割出的同类物体，进一步分割底层组成部分。在几何理解方面，SLAM 早期应用在机器人领域，以出发地点为起始位置，在运动过程中通过重复观测到的地图特征来定位自身位置和姿态，再根据自身位置增量构建地图，达到同时定位和地图构建的目的。**在 XR 领域，SLAM 广泛用于 Inside-Out 追踪定位中**。

**3D 重建**，在**数据采集方面**，由于早期发展受到深度图像传感（RGBD）器件功耗和精度的限制，环境重建技术门槛较高。随着 OPPO、三星华为等主流手机厂商旗舰机型上预制深度相机，激光雷达大幅降价，以及微软发布的 Kinect V4 版本可提供 720P 高精度深度图，使得低成本、高速率生成可用于 VR/AR 的高质量 3D 模型成为可能，对周边环境和物体的理解和建模逐渐平民化。基于 RGBD 相机的动态语义化重建技术逐渐成熟，针对人体形状、运动、材质不易描述等难点，基于参数化人体模型和人体语义分割的语义化分层人体表达、约束及求解方式，在提升人体三维重建精度的同时，实现了人体动态三维信息的多层语义化重建。在**数据处理方面**，随着 AI 能力的渗透释放，2019 年学界出现较多基于单目 RGB 进行深度估计、人体建模、环境建模的学术论文，并开始快速进行技术产业化推进。AI 与三维重建技术的融合创新使二维到三维图像转化以及三维场景理解成为可能。通过海量真实三维重建数据的训练，能够实现单目深度图像估算，通过二维照片估算出真实空间的三维深度数据，从而生成准确的 3D 模型。借助**点云金字塔模型**提取出三维点云在多个尺度上的局部特征，再通过图模型的三维点云语义分割和特征聚合，可完成三维点云体素级别的分类并**最终实现基于三维点云数据的场景理解**。

**3.3 沉浸式声场**

沉浸声场尚待发掘，如**听音辨位、空间混响、通感移觉**等成为发展重点。虚拟现实沉浸体验的进阶提升有赖于对视觉、听觉等多感官通道一致性与关联性的强化。由于周边环境、头耳构型等多重因素会影响双耳听觉闻声辨位，人们通过转头寻视声源，以消除定位判定的模糊性。虚拟现实可结合用户头部追踪特性，解决数字内容长久以来双耳听觉的问题。基于多通道 3D 全景声场拾音技术（Ambisonics），声音表现可依据用户头动情况进行动态解码，虚拟现实用户即可实现更加精准的听音辨位。另外，耳机佩戴致使 3D 全景声被 “压扁”，如何解决因声音高低位置出现的辨位失真成为关键问题。目前，各大巨头对沉浸声场积极投入，并结合人体 3D 扫描开始构建差异化的头部相关传递函数（HRTF）数据库，旨在进一步实现虚拟现实声音的 “私人定制”。由于游戏等应用仅可准确渲染直达声，缺少对房间声学中早期反射和混响的逼真模拟。在混响声模拟技术方面，以往开发人员须将混响手工添加至虚拟环境中的各个位置，操作修改复杂耗时，对算力与内存资源需求较高，且因各声学响应预先计算，仅用于结构保持固定的静态环境。

当前，Facebook 等企业在房间声学上取得了一定成果，混响声可根据环境的几何形状自动精准生成，且符合实时虚拟现实应用严格要求的计算和内存预算，同时实现了随环境空间构型变化的动态混响声模拟，如 VR 密室等探秘游戏。

**总体看感知交互领域国内外存在一定差距，且差距呈现扩大趋势。**

**注视点渲染**
---------

渲染处理主要涉及两部分，一是**内容渲染（生产过程）**，在内容制作过程中将三维虚拟空间场景投影到平面形成平面图像的过程。二是**终端渲染（展示过程）**，即对内容渲染生成的平面图像进行光学畸变、色散校正，以及根据用户姿态进行插帧的处理过程。所有的渲染技术旨在提升渲染性能，以小的开销来渲染更高分辨率、达成用户可感知的细节内容。其中，VR 渲染关键在于复杂的内容运算，如两倍于普通 3D 应用的 GPU 运算量、实时光影效果等，AR（MR）渲染技术与 VR 基本一致，但应用场景侧重于与现实世界的融合，如虚实遮挡、光影渲染、材质反光渲染等。未来，虚拟现实渲染技术将持续向更加丰富、逼真的沉浸体验方向发展，因此，在硬件能力、成本和功耗制约及 2020 年前后 5G 商用的情况下，**注视点渲染、云端渲染、渲染专用芯片、光场渲染**等有望成为业界主流。

**4.1 注视点渲染**

注视点渲染（Foveated Rendering）基于人眼由中心向外围视觉感知逐渐模糊的生理特性，搭配眼球追踪技术，**在不影响用户体验的情况下，显著降低注视点四周的渲染负载**，最多可减少近 80% 画面渲染。除注视点渲染突出的技术成效外，由于该技术与 MultiView、多分辨率渲染、眼球追踪、实时路径追踪、注视点传输以及可减少视觉伪影的注视点图像处理等热点技术交织关联。

**基于眼球追踪的注视点渲染**与注视点光学成为热点技术架构。由于提供高分辨率与色彩视觉的视锥细胞集中分布在人眼最中心区域（Fovea），眼球中央向外的区域视觉感知加速模糊（30° 内每远离 2.5° 视觉分辨率降低一半），业界据此提出注视点渲染技术，通过对视场角内各部分画面进行差异化渲染，显著节省算力开销。2020 年 10 月 Facebook 发售第二代 VR 一体机 Quest 2，新增动态固定注视点功能（Dynamic Fixed Foveated Rendering, DFFR），系统可根据 GPU 帧率高低自动决定是否触发固定注视点渲染。此外，**注视点光学**通过组合低分辨率 / 大 FOV（60+°）与高分辨率 / 小 FOV（20°）两个显示系统，且以手机面板与微显示器或两个不同分辨率的微机电（MEMS）扫描显示系统为常见搭配，旨在实现用户体验分辨率不因渲染算力与显示像素数减少而降低。当前，注视点渲染与注视点光学日益成为支撑上述目标的焦点性技术架构已成为时下技术产业化的主攻方向。

![](https://pic3.zhimg.com/v2-7c4f3772a5df32bf782d19d1c5084fbe_r.jpg)

**4.2 云渲染**

云渲染聚焦云网边端的协同渲染，**时延不确定性**成为关键技术挑战。将虚拟现实交互应用所须的渲染能力导入云端，有助于降低终端配置成本，帮助用户在移动头显平台获得媲美高价 PC 级的渲染质量。

在云化架构的引领下，各类内容应用可更便捷地适配差异化的终端设备，也有助于实施更严格的内容版权保护措施，遏制内容盗版，缓解用户体验痛点清单中的部分问题。本地渲染与云渲染并非完全对立，相比单机版渲染依赖终端完成，云渲染并非完全依靠云侧进行，要解决的是云网边端协同分工，旨在实现云网协同。当前，针对时延、带宽、丢包、抖动等技术挑战，业界通过调节 CPU 与 GPU 协同编码、前向纠错率、缓冲区大小等方式实现 QoS 保障。除流媒体 QoS 视角外，ATW/ASW 成为虚拟现实渲染标配的 “弃帧保险”，由于 ATW 导致视觉黑边，可通过扩大渲染面积予以解决。

此外，在虚拟现实体验过程中用户即便没有位移，眼睛亦会发生位置改变，因而引入 ASW，前者适用于远景静物，后者侧重近景动画。下图为中国移动 5G 联合创新中心《5G 云 XR 端到端能力需求研究报告》给出的数据。

![](https://pic3.zhimg.com/v2-67450cd9bd87bb36bed389380cbde826_r.jpg)

**4.3 人工智能**

人工智能将成为虚拟现实渲染质量与效能的倍增器与调和剂。当前，业界日益聚焦深度学习渲染这一热点领域，以期针对多样化的业务场景，解锁平衡质量、速度、能耗、带宽、成本等多维渲染指标间的技术定式。

在**渲染质量**方面，比之基于传统渲染软硬件架构的超采样（SSAA）、多重采样（MSAA）、快速近似（FXAA）、子像素增强（SMAA）、覆盖采样（CSAA）、时间性抗锯齿（TXAA）等抗锯齿技术，在 2018 年英伟达发布 GeForce RTX 20 系列显卡中，推出了包含深度学习超采样（DLSS）功能的驱动程序，通过以较低分辨率渲染图像再经 AI 算法填充像素的方式，显著提升了画面精细程度。

在**渲染效能**方面，为在移动终端平台加载高质量的虚拟现实沉浸体验，业界结合深度学习与人眼注视点特性，积极探索在不影响画质感知的情况下，如何进一步优化渲染效能的技术路径。脸书提出一种基于 AI 的注视点渲染系统 DeepFovea，利用生成对抗网络（GAN）的新近研究进展，通过馈送数百万个真实视频片段模拟注视点外围像素密度降低来训练 DeepFovea 网络，GAN 的设计有助于神经网络根据训练视频的统计信息来补缺细节，进而得到可基于稀疏输入生成自然视频片段的渲染系统。测试显示该方案可将渲染计算负载降低约十倍，且能够管理外围视场的闪烁、锯齿和其他视频伪影。

在**图像预处理**方面，预先对图像进行降噪处理有助于提升后续图像分割、目标识别、边缘提取等任务的实际效果，与传统降噪方法相比，深度学习降噪可获得更优的峰值信噪比（PSNR）与结构相似性（SSIM），如英伟达 OptiX 6.0 采用人工智能加速高性能降噪处理，从而减少高保真图像渲染时间。

在**端云协同**架构方面，随着电信运营商云化虚拟现实发展推广，针对多样化的应用场景与网络环境，人工智能有望成为渲染配置自优化的重要探索。

总体看，**在渲染领域，国内外差距显著**，当前注视点技术和人工智能方面主要是跟随为主，但是国内已经有企业开始积极投入。

**网络传输**
--------

面向虚拟现实的网络传输强调基于针对虚拟现实**带宽、时延双敏感**的业务特性，优化适配各类网络传输技术，探索网联式云化虚拟现实技术路径，旨在保证不断进阶视觉沉浸性与内容交互性的同时，着力提升用户使用移动性，降低大众软硬件购置成本，加速虚拟现实普及推广。与 VR 相比，由于 AR 侧重与真实环境的人机交互，须将摄像头捕捉到的图片 / 视频上传云端，云端实时下载需要增强叠加显示的虚拟信息，因此**需求更多的上行带宽**。鉴于虚拟现实网络传输涉及接入网、承载网、数据中心、网络运维与监控及投影、编码压缩等技术领域，有关技术产业化进程。

和超高清类似在**接入网、承载网**等方面相关技术，如网络切片，边缘计算等在此不详述，只讨论 XR 相关的部分。

在数据编解码传输方面，**传输预处理**，目前虚拟现实视频编码仍主要使用 HEVC，针对 VR 360 度视频的编码已经标准化，编码工具已经成熟。MPEG 等标准组织的研究表明，对应于 **HEVC** 的下一代编码技术（H.266）的压缩效率可提升 30%。VR 传输方案两种方式，一种是**全视角（等质量）传输**，终端接收到的一帧数据中包含了用户可看到的空间球对应的全部视角信息，属于用 **“带宽换时延”** 的做法，很大一部分传送到用户端的内容数据被浪费。另一种是 **FOV 传输技术**渐渐成为主流，终端接收到的一帧数据是根据用户视角姿态构造对应的帧数据，终端判断用户转头改变视角的姿态位置，并发送至云端，请求新姿态对应的帧数据。该技术对带宽要求降低，时延要求变高，属于 **“时延换带宽”**。现阶段 **FOV 传输技术存在以下三条发展路径**，一是 Facebook 提出的**金字塔模型**，即在内容准备侧，针对每个视角准备一个全视角的质量不均匀的码流，模型底部为高质量用户视角区域，随着金字塔高度的上升，其他区域通过亚采样降低分辨率。终端根据用户当前视角姿态位置， 向服务器请求对应的视角文件。缺点是多耗费头端 GPU 编码、CDN 存储和传输带宽。二是**基于视频分块（Tile）的 TWS 传输方案**，在内容准备侧，将 VR 画面划分为多个 Tile，每个区域对应一个可以独立解码的码流，同时准备一个低质量全视角的 VR 码流，根据用户视点和视角只传输观看范围内容的高质量 Tile 视频分块和最低质量全视角视频。该方案被 MPEG 组织 OMAF 工作组采纳，并写入了新近标准文档《ISO/IEC FDIS 23090-2 Omnidirectional Media Format》中，被推荐采用。三是 **FOV + 方案**，FOV + 不是全视角编码，而是不同视点的剪切视频流编码，通过传输比 FOV 角度略大画面来应对网络和处理时延，降低交互体验对网络的要求。

总体看，**在网络传输方面我国处在世界领先水平**。

以上从近眼显示、内容制作、感知交互、渲染处理、网络传输等方面对 XR 关键技术的分析，深度有限，内容相对较全面。**总体看国内在感知交互和渲染处理领域发展滞后于国外**，尤其**技术前瞻预研方面，需要大力加强**。后续周刊中会对 XR 应用场景、产业情况以及未来趋势继续讨论。

参考资料

[1] 3GPP TR 26.928. “Extended Reality (XR) in 5G”, 2020.  

[2] 虚拟（增强）现实白皮书（2017） CAICT 中国信通院.

[3] 虚拟（增强）现实白皮书（2018） CAICT 中国信通院.

[4] 虚拟（增强）现实白皮书（2021） CAICT 中国信通院.

[5] cloud VR + 产品白皮书 中国移动.

[6] AR 边缘云白皮书技术概览（移动和中兴）.

[7] The mobile future of extended reality）（XR）(高通).

[8] 微软《混合现实文档》

[https://docs.microsoft.com/zh-cn/windows/mixed-reality/](https://link.zhihu.com/?target=https%3A//docs.microsoft.com/zh-cn/windows/mixed-reality/).