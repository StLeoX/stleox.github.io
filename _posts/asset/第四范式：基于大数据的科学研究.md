> from [zhuanlan.zhihu.com](https://zhuanlan.zhihu.com/p/79346043) 

图灵奖得主，关系型数据库的鼻祖吉姆 · 格雷 (Jim Gray) 也是一位航海运动爱好者。2007 年 1 月 28 日，他驾驶帆船在茫茫大海中失联了。而就是 17 天前的 1 月 11 日，在加州山景城召开的 NRC-CSTB（National ResearchCouncil-Computer Science and Telecommunications Board）大会上，他发表了留给世人的最后一次演讲“科学方法的革命”，提出将科学研究分为四类范式（Paradigm，某种必须遵循的规范或大家都在用的套路），依次为**实验归纳，模型推演，仿真模拟和数据密集型科学发现**（Data-IntensiveScientific Discovery）。其中，**最后的 “数据密集型”，也就是现在我们所称的 “科学大数据”。**

人类最早的科学研究，主要以记录和描述自然现象为特征，称为 **“实验科学”（第一范式）**，从原始的钻木取火，发展到后来以伽利略为代表的文艺复兴时期的科学发展初级阶段，开启了现代科学之门。

但这些研究，显然受到当时实验条件的限制，难于完成对自然现象更精确的理解。科学家们开始尝试尽量简化实验模型，去掉一些复杂的干扰，只留下关键因素（这就出现了我们在学习物理学中 “足够光滑”、“足够长的时间”、“空气足够稀薄” 等令人费解的条件描述），然后通过演算进行**归纳总结，这就是第二范式**。这种研究范式一直持续到 19 世纪末，都堪称完美，牛顿三大定律成功解释了经典力学，麦克斯韦理论成功解释了电磁学，经典物理学大厦美轮美奂。但之后量子力学和相对论的出现，则以理论研究为主，以超凡的头脑思考和复杂的计算超越了实验设计，而随着验证理论的难度和经济投入越来越高，科学研究开始显得力不从心。

20 世纪中叶，冯 · 诺依曼提出了现代电子计算机架构，利用电子计算机对科学实验进行模拟仿真的模式得到迅速普及，人们可以对复杂现象通过模拟仿真，推演出越来越多复杂的现象，典型案例如模拟核试验、天气预报等。随着**计算机仿真**越来越多地取代实验，逐渐成为科研的常规方法，即**第三范式**。

而未来科学的发展趋势是，随着数据的爆炸性增长，计算机将不仅仅能做模拟仿真，还能进行分析总结，得到理论。**数据密集范式理应从第三范式中分离出来，成为一个独特的科学研究范式**。也就是说，过去由牛顿、爱因斯坦等科学家从事的工作，未来完全可以由计算机来做。这种科学研究的方式，被称为**第四范式**。

我们可以看到，第四范式与第三范式，都是利用计算机来进行计算，二者有什么区别呢？现在大多科研人员，可能都非常理解第三范式，在研究中总是被导师、评委甚至是自己不断追问 “科学问题是什么？”，“有什么科学假设？”，这就是先提出可能的理论，再搜集数据，然后通过计算来验证。而基于大数据的第四范式，则是先有了大量的已知数据，然后通过计算得出之前未知的理论。在维克托 · 迈尔 - 舍恩伯格撰写的《大数据时代》（中文版译名）中明确指出，**大数据时代最大的转变，就是放弃对因果关系的渴求，取而代之关注相关关系**。也就是说，只要知道 “是什么”，而不需要知道 “为什么”。这就颠覆了千百年来人类的思维惯例，据称是对人类的认知和与世界交流的方式提出了全新的挑战。因为人类总是会思考事物之间的因果联系，而对基于数据的相关性并不是那么敏感；相反，电脑则几乎无法自己理解因果，而对相关性分析极为擅长。这样我们就能理解了，**第三范式是 “人脑 + 电脑”，人脑是主角，而第四范式是 “电脑 + 人脑”，电脑是主角。**这样的一种说法，显然遭到了许多人的反对，认为这是将科学研究的方向领入歧途。从科学论文写作角度来说，如果通篇只有对数据相关性的分析，而缺乏具体的因果解读，这样的文章一般被认为是数据堆砌，是不可能发表的。

然而，要发现事物之间的因果联系，在大多数情况下总是困难重重的。**我们人类推导的因果联系，总是基于过去的认识，获得 “确定性” 的机理分解，然后建立新的模型来进行推导。但是，这种过去的经验和常识，也许是不完备的，甚至可能有意无意中忽略了重要的变量。**

这里举一个大家容易理解的例子。现在我们人人都在关注雾霾天气。我们想知道：雾霾天气是如何发生的，如何预防？首先需要在一些 “代表性” 位点建立气象站，来收集一些与雾霾形成有关的气象参数。根据已有的机理认识，雾霾天气的形成不仅与源头和大气化学成分有关，还与地形、风向、温度、湿度气象因素有关。仅仅这些有限的参数，就已经超过了常规监测的能力，只能进行简化人为去除一些看起来不怎么重要的，只保留一些简单的参数。那些看起来不重要的参数会不会在某些特定条件下，起到至关重要的作用？如果再考虑不同参数的空间异质性，这些气象站的空间分布合理吗，足够吗？从这一点来看，如果能够获取更全面的数据，也许才能真正做出更科学的预测，这就是第四范式的出发点，也许是最迅速和实用的解决问题的途径。

那么，第四范式将如何进行研究呢？多年前说这个话题，也许许多人会认为是天方夜谭，但目前在移动终端横行和传感器高速发展的时代，未来的趋势似乎就在眼前了。现在，我们的手机可以监测温度、湿度，可以定位空间位置，不久也许会出现能监测大气环境化学和 PM2.5 功能的传感设备，这些移动的监测终端更增加了测定的空间覆盖度，同时产生了海量的数据，利用这些数据，分析得出雾霾的成因，最终进行预测也许指日可待。

这种海量数据的出现，不仅超出了普通人的理解和认知能力，也给计算机科学本身带来了巨大的挑战。因此，当大这些规模计算的数据量超过 1PB 时，传统的存储子系统已经难以满足海量数据处理的读写需要，数据传输 I/O 带宽的瓶颈愈发突出。而简单地将数据进行分块处理并不能满足数据密集型计算的需求，与大数据分析的初衷是相违背的。因此，目前许多在具体研究中所面临的最大问题，不是缺少数据，而是面对太多的数据，却不知道如何处理。目前可见的一些技术，比如超级计算机、计算集群、超级分布式数据库、基于互联网的云计算，似乎并没有解决这些矛盾的核心问题。计算机科学期待新的革命！